{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Quick_CO2_with_geometry_for_all_projects_in_a_folder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtUmsK9Cyqh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1befff55-50c3-4655-8924-0f951f539046"
      },
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import xml.etree.ElementTree as ET\n",
        "from slugify import slugify\n",
        "import requests\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "# Determining the path where the files are located and creating a new folder\n",
        "path = '/content/IFC2'\n",
        "pathn = path + '/quick_rough_estimate'\n",
        "try:\n",
        "  os.mkdir(pathn)\n",
        "except:\n",
        "  pass\n",
        "  \n",
        "# Properties for which we want to collect data on the amount of volume\n",
        "propstr = ['Area', 'Volume']\n",
        "\n",
        "# Main function for grouping data and saving a file\n",
        "def crtable(filename):\n",
        "    filenamep = path + '/' + filename\n",
        "    df = pd.read_csv(filenamep, low_memory=False)\n",
        "    filedae = path + '/' + filename[:-8]+'dae'\n",
        "    \n",
        "    #  Fetching only numbers from string values of volumetric parameters\n",
        "    propindf = []\n",
        "    \n",
        "    #grouping by element types for different formats\n",
        "    if 'ifc' in filenamep:\n",
        "      search_parameter = 'Type'\n",
        "    else:\n",
        "      search_parameter = 'Type Name'\n",
        "\n",
        "    # Converting all \"propstr\" values in columns to numeric values\n",
        "    for el in propstr:\n",
        "      if el in df.columns:\n",
        "        propindf.append(el)\n",
        "    def find_number(text):\n",
        "      num = re.findall(r'[0-9]+', text)\n",
        "      return \".\".join(num)\n",
        "    for el in propindf:\n",
        "      df[el] = df[el].astype(str)\n",
        "      df[el] = df[el].apply(lambda x: find_number(x))\n",
        "      df[el] = pd.to_numeric(df[el], errors='coerce')\n",
        "      df[el] = df[el].replace(np.nan, 0)\n",
        "      df[el] = df[el].replace('None', 0)\n",
        "      df[el] = df[el].fillna(0)\n",
        "    try:\n",
        "        df[el] = df[el].astype(float)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Summation of all data that are grouped by search_parameter located in the propindf columns\n",
        "    df1=pd.pivot_table(df, index=[search_parameter],values=propindf,aggfunc=np.sum)\n",
        "    df1 = df1.add_prefix('Sum of ')\n",
        "\n",
        "    # Determination of the number of elements in groups\n",
        "    df2= df.groupby([search_parameter])[propindf[0]].agg(['count'])\n",
        "    dfallpar = pd.DataFrame()  \n",
        "    df['Unnamed: 0'] = df['Unnamed: 0'].astype(str)\n",
        "    comma = lambda x: ', '.join(x.unique())\n",
        "    df3 = df.groupby([search_parameter]) .agg({'Unnamed: 0': comma})\n",
        "    \n",
        "    # Collecting data into one dataframe\n",
        "    dfallpar = pd.concat([df2, df1, df3], axis=1)\n",
        "    dfallpar.rename(columns=({ 'Unnamed: 0': 'Id´s', 'count': 'Amount'}), inplace=True,)\n",
        "    \n",
        "    # Formula to add to the HLS table to automatically find values in CG for a group\n",
        "    formeldba = \" =WENNFEHLER(INDEX('Info'!$F$6:$F$17,VERGLEICH($E\"\n",
        "    formeldbe = \",'Info'!$C$6:$C$17,0),VERGLEICH(F$1,'Info'!$F$4:$F$4,0)),0)\"\n",
        "\n",
        "    # Creating unique formulas for each row\n",
        "    forarr = []\n",
        "    forarrm = []\n",
        "    anzr = len(dfallpar.index) + 2\n",
        "    for iteration, item in enumerate(range(2, anzr, 1)):\n",
        "      forarr.append(formeldba + str(item) + formeldbe)\n",
        "      forarrm.append(\"=D\"+str(item)+\"*F\"+str(item))\n",
        "\n",
        "    # Creating new columns in the Excel file\n",
        "    dfallpar.insert(3, \"The sum of the CO2 emissions for the group, Kg\", forarrm)\n",
        "    dfallpar.insert(3, \"Сoefficient m3 to kg\", forarr)\n",
        "    dfallpar.insert(3, \"CO2 group\", '')\n",
        "    \n",
        "    # Use and download a sample excel file\n",
        "    url = 'https://github.com/OpenDataBIM/CO2_calculating-the-embodied-carbon/raw/main/Sample_Fossil-Carbon-Emitted-in-Production.xlsx'\n",
        "    r = requests.get(url)\n",
        "    excelf = pathn + '/' + 'CO2_' + filename+'.xlsx'     \n",
        "    with open(excelf, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "\n",
        "    # Saving data to file\n",
        "    book = load_workbook(excelf)\n",
        "    writer = pd.ExcelWriter(excelf, engine='openpyxl') \n",
        "    writer.book = book\n",
        "    writer.sheets = dict((ws.title, ws) for ws in book.worksheets)\n",
        "    dfallpar.to_excel(writer, 'CO2 group')\n",
        "    writer.save()\n",
        "\n",
        "    dfallpar['type'] = dfallpar.index\n",
        "    dfallpar['type'] = dfallpar['type'].apply(slugify)\n",
        "    print(\"File created: \" + excelf)\n",
        "    \n",
        "    # Start sorting geometry from DAE file\n",
        "    # Formation of a data tree from the DAE format\n",
        "    daegrpath = pathn + '/' + 'DAEgroups_' + filename[:-9] \n",
        "    try:\n",
        "      os.mkdir(daegrpath)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    # If the ID of an element from the group_ids_str list that was found earlier matches,\n",
        "    # all elements with this ID are found in the DAE file, and all other elements are deleted\n",
        "    filedaearr = []\n",
        "    for index, row in dfallpar.iterrows():\n",
        "      fileObject = open(filedae, \"r\")\n",
        "      treeq = ET.parse(fileObject)\n",
        "      root = treeq.getroot()\n",
        "      ET.register_namespace(\"\", \"http://www.collada.org/2005/11/COLLADASchema\")\n",
        "      geom_list = []\n",
        "      group_ids_str = []\n",
        "      group_ids_str = re.findall(r'\\d+', row['Id´s'])\n",
        "      for node in root.findall('.//{http://www.collada.org/2005/11/COLLADASchema}node'):\n",
        "          tree = treeq\n",
        "          if node.attrib['id'] in group_ids_str:\n",
        "            try:\n",
        "              url = list(node)[0].get('url')\n",
        "              geom_list.append(url[1:])\n",
        "            except:\n",
        "              pass\n",
        "          else:\n",
        "              try:\n",
        "                  nd = node.find(\n",
        "                      '{http://www.collada.org/2005/11/COLLADASchema}instance_geometry')\n",
        "                  node.remove(nd)\n",
        "              except:\n",
        "                  0\n",
        "      for geomet in root.findall('.//{http://www.collada.org/2005/11/COLLADASchema}geometry'):\n",
        "            if geomet.attrib['id'] in geom_list:\n",
        "                0\n",
        "            else:\n",
        "                try:\n",
        "                  md = geomet.find(\n",
        "                      '{http://www.collada.org/2005/11/COLLADASchema}mesh')\n",
        "                  geomet.remove(md)\n",
        "                except:\n",
        "                  pass\n",
        "\n",
        "      # Formation of a new name for the DAE file with grouped elements\n",
        "      #words_pattern = '[a-zA-Z10-9]+'\n",
        "      invalid = '<>:\"/\\|?* '\n",
        "      for char in invalid:\n",
        "        index = index.replace(char, '')\n",
        "      regw = index + '.dae'\n",
        "      filedaena = daegrpath + '/' + regw\n",
        "      with open(filedaena, 'w') as f:\n",
        "          tree.write(f, encoding='unicode')\n",
        "      #filedaearr.append(\"\"\"=HYPERLINK(\"[\"\"\"+\"/\" + \"DAEgroups_\" + filename[:-9] + \"/\" + regw + \"]\" + regw +\"\"\"\")\"\"\")\n",
        "      filedaearr.append('=HYPERLINK(LEFT(CELL(\"filename\",A1),FIND(\"[\",CELL(\"filename\",A1))-1)&\"' + \"DAEgroups_\" + filename[:-9] + '\\\\' + regw +'\",\"'+ regw + '\")')\n",
        "    dfallpar.drop(columns=['type'])\n",
        "    dfallpar.insert(6, \"Group geometry in DAE, file hyperlink *.dae)\", filedaearr)\n",
        "    with open(excelf, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "\n",
        "    # Saving data to file\n",
        "    book = load_workbook(excelf)\n",
        "    writer = pd.ExcelWriter(excelf, engine='openpyxl') \n",
        "    writer.book = book\n",
        "    writer.sheets = dict((ws.title, ws) for ws in book.worksheets)\n",
        "    dfallpar.to_excel(writer, 'CO2 group')\n",
        "    writer.save()\n",
        "\n",
        "# Function execution cycle for all CSV files in the folder\n",
        "for filename in os.listdir(path):\n",
        "  if filename.endswith(\"csv\"): \n",
        "      crtable(filename)\n",
        "      \n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "# Saving data to a ZIP file for downloading to a computer\n",
        "#!zip -r /content/file.zip /content/IFC2/quick_rough_estimate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File created: /content/IFC2/quick_rough_estimate/CO2_ID-White BIMJSON Test-R21_rvt.json.csv.xlsx\n",
            "--- 15.187286376953125 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkVEb1tO5loi",
        "outputId": "4eaa5d35-da15-4085-884f-c6b98d04e9a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WXTv8BF45mH4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}